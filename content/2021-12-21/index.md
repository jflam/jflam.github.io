+++
title="2021-12-21"
date=2021-12-21
+++

**Day 3 of fastai**. The
[book](https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527/ref=sr_1_3?crid=39HBTBCBZ94GK&keywords=fastai+book&qid=1640122478&sprefix=fastai+bo%2Caps%2C372&sr=8-3)
arrived today and I'm following along from the book directly. A combination of
reading the paper book and using the Jupyter notebooks found in the [GitHub
book repo](https://github.com/fastai/fastbook) are a potent combination.
Reminder that you can run the book repo in just a single step using
[ez](https://github.com/jflam/ez) and VS Code.

The [Universal Approximation
Theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem)
undergirds the theoretical basis for neural networks being able to compute
arbitrary functions. The two parts of the approximation theorem look at the
limits of a single layer with an arbitrary number of neurons ("arbitrary
width") and the limits of a network with an arbitrary number of hidden layers
("arbitrary depth").

